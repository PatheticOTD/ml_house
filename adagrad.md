![[Screenshot from 2024-08-05 17-52-38.png]]

где 
$\eta$- learning rate
$\epsilon$ - стабилизатор, который дает уверенность в том, что мы не разделим на ноль"


Адаград принимает во внимание предыдущие градиенты.

Главный минус таких методов