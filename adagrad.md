![[Screenshot from 2024-08-05 17-52-38.png]]

где 
$\eta$- learning rate
$\epsilon$ - стабилизатор, который дает уверенность в том, что мы не разделим на ноль"


Подход адаграда схож с другими методами убывающего изменения learnin rate-а
Главная проблема таких методов состоит в том, что они могут просто не добраться до минимума, если он очень далеко, из-за убывания лернинг рейта. 
Адаград же 