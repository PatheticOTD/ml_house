это отдельные эмбеддинги, которые моделька учит вместе с ембедингами слов.

Для трансформера, авторы статьи предложили кодировать позиции следующим образом: ![[Pasted image 20250120113717.png]]

где t - позиция слова в предложении