$\delta_j^l$ - ошибка на j-том нейроне l-ого слоя. 
Метод обратного распространения ошибки поможет нам вычислить эту самую ошибку и затем соотнести ее с $\frac{dC}{dw_{jk}^l}$ и с такой же производной по bias - у

Чем больше эта ошибка, тем больше производная $\frac{dC}{dz_j^l}$ и наоборот. Соответственно ошибка эквивалентна производной C по z j-ого нейрона l-ого слоя.
$\delta^l$ -  вектор ошибок слоя l. бэкпропогэтион даст нам способ вычислить этот самый вектор для каждого слоя и затем соотнести эти ошибки с частными производными  $\frac{dC}{dw_{jk}^l}$ и такой же производной по bias - у.

#### 1) Уравнение ошибки


![[Screenshot from 2024-08-14 16-53-24.png]]
